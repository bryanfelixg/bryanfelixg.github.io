<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Sampling</title>

  <!-- Style Sheet & Font-->
  <link rel="stylesheet" href="../css/stylecode.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&display=swap" rel="stylesheet">
  
  <!-- Navigation Sidebar Script -->
  <script defer src="../jss/navigation.js"></script>

  <!-- SageMath Library -->
  <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
  <script>
    // Make *any* div with class 'sage' a Sage cell
    sagecell.makeSagecell({inputLocation: 'div.sage',
                           evalButtonText: 'Run',
                           editor: "codemirror",
                           languages: ['python'],
                           linked: 'true'});
  </script>

  <!-- MathJax for LaTeX -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<div class="container"> 
<div class="sidebar">
<ul></ul> 
</div>
    
<article id="post-content">  
<header>
<h1 id="title"> Sampling Methods in Python </h1>
<p class="author-subtitle">Bryan Félix</p> 
<time datetime="2024-08-15">Aug 19, 2024</time>
</header>

<h2 id="intro">Introduction</h2>
<p>This is a quick introduction to sampling methods for machine learning applications. The structure of the notebook is as follows. First we create an artificial set of data to model. Then we will create a <em>polynomial regression</em> model to use as template with arbitrary degree. Each of the following sampling methods will be implemented in python: <em>random sampling</em>, <em> \(k-\)fold cross-validation</em>, and, <em>bootstraping</em>. For each we will compute the <em>test error rate</em> and compare the variance of the results.</p>

<h2 id="data">Simulating some data</h2>
<p>For exposition, lets simulate data points that <em>"look"</em> cubic. Lets take a few points in the interval \(x \in [-3,-3]\) and map them to the function \[f(x) = x^3 -2x^2 +x +3 + \varepsilon. \]
Where \(\varepsilon\) is random noise sampled from a normal distribution with mean (\(\mu\)) zero, and standard deviation (\(\sigma\)) equal to 2. 
</p>

<div class="sage">
<script type="text/sage">
import numpy as np
𝜇 = 0 
𝜎 = 2
𝜀 = np.random.normal(loc = 𝜇, scale = 𝜎, size = 100)

x = np.linspace(-3,3,100)
y = x**3 - 2*x**2 + x + 3 + 𝜀

print(f"x: {x[:3]}...\ny: {y[:3]}...")
</script>
</div>
<br>
<div class="sage">
<script type="text/sage">
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_title("Simulated Data")
ax.set_xlabel("x")
ax.set_ylabel("f(x) = x^3 - 2x^2 + x + 3")
plt.show()
</script>
</div>

<h2 id="model">Polynomial Regression with Sci-Kit Learn</h2>
<p>Lets assume we do <strong>not</strong> know the degree of the data <em>a priori</em>. Therefore, lets build a general linear polynomial model that allows us to use varying degrees. The method here is described in more detail at <a href="https://www.geeksforgeeks.org/python-implementation-of-polynomial-regression/">Geeks for Geeks</a>. However, the intuition is to <em>"engineer"</em> the features \(1, x, x^2, x^3\) for each value \(x\) and then fit a <em>linear regression model</em> into the new features. In technical notation, we are looking for coefficients \(\beta_0,\beta_1, \beta_2, \beta_3\) that <strong>best</strong> approximate the solution for the following equation: \[
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix} 
=
\begin{bmatrix}
1 & x_{1} & x_{1}^2 & x_{1}^3 \\
1 & x_{2} & x_{2}^2 & x_{2}^3 \\
\vdots & \vdots & \vdots & \vdots \\
1 & x_{n} & x_{n}^2 & x_{n}^3
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3
\end{bmatrix}
.\]
Where \(x_i\) is the data point with value \(y_i\).</p>

<div class="sage">
<script type="text/sage">
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

def polynomial_model(degree, x_train, y_train):
    x_train = x_train.reshape(-1,1)
    poly = PolynomialFeatures(degree = degree)
    x_poly = poly.fit_transform(x_train)
    model = LinearRegression()
    model.fit(x_poly, y_train)
    return model, poly
</script>
</div>

</article>

</div>

</body>
</html>
