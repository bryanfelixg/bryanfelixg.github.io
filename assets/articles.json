{
    "last_month": [
        {
            "title": "Latent Diffeomorphic Dynamic Mode Decomposition",
            "year": "May 2025",
            "date": "2025-05-09",
            "authors": "Willem Diepeveen, Jon Schwenk, Andrea Bertozzi",
            "abstract": "We present Latent Diffeomorphic Dynamic Mode Decomposition (LDDMD), a new\ndata reduction approach for the analysis of non-linear systems that combines\nthe interpretability of Dynamic Mode Decomposition (DMD) with the predictive\npower of Recurrent Neural Networks (RNNs). Notably, LDDMD maintains simplicity,\nwhich enhances interpretability, while effectively modeling and learning\ncomplex non-linear systems with memory, enabling accurate predictions. This is\nexemplified by its successful application in streamflow prediction.",
            "link": "http://arxiv.org/pdf/2505.06351v1"
        },
        {
            "title": "Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data",
            "year": "May 2025",
            "date": "2025-05-13",
            "authors": "Antonio Álvarez-López, Marcos Matabuena",
            "abstract": "Modeling the continuous--time dynamics of probability distributions from\ntime--dependent data samples is a fundamental problem in many fields, including\ndigital health. The aim is to analyze how the distribution of a biomarker, such\nas glucose, evolves over time and how these changes may reflect the progression\nof chronic diseases such as diabetes. In this paper, we propose a novel\nprobabilistic model based on a mixture of Gaussian distributions to capture how\nsamples from a continuous-time stochastic process evolve over the time. To\nmodel potential distribution shifts over time, we introduce a time-dependent\nfunction parameterized by a Neural Ordinary Differential Equation (Neural ODE)\nand estimate it non--parametrically using the Maximum Mean Discrepancy (MMD).\nThe proposed model is highly interpretable, detects subtle temporal shifts, and\nremains computationally efficient. Through simulation studies, we show that it\nperforms competitively in terms of estimation accuracy against\nstate-of-the-art, less interpretable methods such as normalized gradient--flows\nand non--parameteric kernel density estimators. Finally, we demonstrate the\nutility of our method on digital clinical--trial data, showing how the\ninterventions alters the time-dependent distribution of glucose levels and\nenabling a rigorous comparison of control and treatment groups from novel\nmathematical and clinical perspectives.",
            "link": "http://arxiv.org/pdf/2505.08698v1"
        },
        {
            "title": "Heteroclinic Connection in a Nicholson's delayed model with Harvesting term",
            "year": "May 2025",
            "date": "2025-05-16",
            "authors": "Adrian Gomez, Cesar Guayasamin",
            "abstract": "In this paper we prove the existence of monotone heteroclinic solutions for\nthe delayed Nicholson's blowflies model with harvesting:\n  \\[\n  x'(t) = -\\delta x(t) - Hx(t-\\sigma) + \\rho x(t-r)e^{-x(t-r)}.\n  \\]\n  Under the condition \\(1 < \\dfrac{\\rho}{\\delta+H} \\leq e\\), we establish the\nconnection between the equilibria \\(0\\) and \\(\\ln(\\rho/(\\delta+H))\\) using the Wu\nand Zou monotone iteration method adapted for two delays (\\(\\sigma \\neq r\\)). The\nproof combines explicit upper and lower solutions construction with\ncharacteristic equation analysis, supported by numerical simulations.",
            "link": "http://arxiv.org/pdf/2505.11657v1"
        },
        {
            "title": "On the emergence of numerical instabilities in Next Generation Reservoir Computing",
            "year": "May 2025",
            "date": "2025-05-01",
            "authors": "Edmilson Roque dos Santos, Erik Bollt",
            "abstract": "Next Generation Reservoir Computing (NGRC) is a low-cost machine learning\nmethod for forecasting chaotic time series from data. However, ensuring the\ndynamical stability of NGRC models during autonomous prediction remains a\nchallenge. In this work, we uncover a key connection between the numerical\nconditioning of the NGRC feature matrix -- formed by polynomial evaluations on\ntime-delay coordinates -- and the long-term NGRC dynamics. Merging tools from\nnumerical linear algebra and ergodic theory of dynamical systems, we\nsystematically study how the feature matrix conditioning varies across\nhyperparameters. We demonstrate that the NGRC feature matrix tends to be\nill-conditioned for short time lags and high-degree polynomials.\nIll-conditioning amplifies sensitivity to training data perturbations, which\ncan produce unstable NGRC dynamics. We evaluate the impact of different\nnumerical algorithms (Cholesky, SVD, and LU) for solving the regularized\nleast-squares problem.",
            "link": "http://arxiv.org/pdf/2505.00846v1"
        },
        {
            "title": "Generative Learning for Slow Manifolds and Bifurcation Diagrams",
            "year": "April 2025",
            "date": "2025-04-29",
            "authors": "Ellis R. Crabtree, Dimitris G. Giovanis, Nikolaos Evangelou, Juan M. Bello-Rivas, Ioannis G. Kevrekidis",
            "abstract": "In dynamical systems characterized by separation of time scales, the\napproximation of so called ``slow manifolds'', on which the long term dynamics\nlie, is a useful step for model reduction. Initializing on such slow manifolds\nis a useful step in modeling, since it circumvents fast transients, and is\ncrucial in multiscale algorithms alternating between fine scale (fast) and\ncoarser scale (slow) simulations. In a similar spirit, when one studies the\ninfinite time dynamics of systems depending on parameters, the system\nattractors (e.g., its steady states) lie on bifurcation diagrams. Sampling\nthese manifolds gives us representative attractors (here, steady states of ODEs\nor PDEs) at different parameter values. Algorithms for the systematic\nconstruction of these manifolds are required parts of the ``traditional''\nnumerical nonlinear dynamics toolkit.\n  In more recent years, as the field of Machine Learning develops, conditional\nscore-based generative models (cSGMs) have demonstrated capabilities in\ngenerating plausible data from target distributions that are conditioned on\nsome given label. It is tempting to exploit such generative models to produce\nsamples of data distributions conditioned on some quantity of interest (QoI).\nIn this work, we present a framework for using cSGMs to quickly (a) initialize\non a low-dimensional (reduced-order) slow manifold of a multi-time-scale system\nconsistent with desired value(s) of a QoI (a ``label'') on the manifold, and\n(b) approximate steady states in a bifurcation diagram consistent with a (new,\nout-of-sample) parameter value. This conditional sampling can help uncover the\ngeometry of the reduced slow-manifold and/or approximately ``fill in'' missing\nsegments of steady states in a bifurcation diagram.",
            "link": "http://arxiv.org/pdf/2504.20375v1"
        }
    ],
    "last_year": [
        {
            "title": "Predator Prey Scavenger Model using Holling's Functional Response of Type III and Physics-Informed Deep Neural Networks",
            "year": "December 2024",
            "date": "2024-12-24",
            "authors": "Aneesh Panchal, Kirti Beniwal, Vivek Kumar",
            "abstract": "Nonlinear mathematical models introduce the relation between various physical\nand biological interactions present in nature. One of the most famous models is\nthe Lotka-Volterra model which defined the interaction between predator and\nprey species present in nature. However, predators, scavengers, and prey\npopulations coexist in a natural system where scavengers can additionally rely\non the dead bodies of predators present in the system. Keeping this in mind,\nthe formulation and simulation of the predator prey scavenger model is\nintroduced in this paper. For the predation response, respective prey species\nare assumed to have Holling's functional response of type III. The proposed\nmodel is tested for various simulations and is found to be showing satisfactory\nresults in different scenarios. After simulations, the American forest dataset\nis taken for parameter estimation which imitates the real-world case. For\nparameter estimation, a physics-informed deep neural network is used with the\nAdam backpropagation method which prevents the avalanche effect in trainable\nparameters updation. For neural networks, mean square error and\nphysics-informed informed error are considered. After the neural network, the\nhence-found parameters are fine-tuned using the\nBroyden-Fletcher-Goldfarb-Shanno algorithm. Finally, the hence-found parameters\nusing a natural dataset are tested for stability using Jacobian stability\nanalysis. Future research work includes minimization of error induced by\nparameters, bifurcation analysis, and sensitivity analysis of the parameters.",
            "link": "http://arxiv.org/pdf/2412.18344v1"
        },
        {
            "title": "ADAM-SINDy: An Efficient Optimization Framework for Parameterized Nonlinear Dynamical System Identification",
            "year": "October 2024",
            "date": "2024-10-21",
            "authors": "Siva Viknesh, Younes Tatari, Amirhossein Arzani",
            "abstract": "Identifying dynamical systems characterized by nonlinear parameters presents\nsignificant challenges in deriving mathematical models that enhance\nunderstanding of physics. Traditional methods, such as Sparse Identification of\nNonlinear Dynamics (SINDy) and symbolic regression, can extract governing\nequations from observational data; however, they also come with distinct\nadvantages and disadvantages. This paper introduces a novel method within the\nSINDy framework, termed ADAM-SINDy, which synthesizes the strengths of\nestablished approaches by employing the ADAM optimization algorithm. This\nfacilitates the simultaneous optimization of nonlinear parameters and\ncoefficients associated with nonlinear candidate functions, enabling precise\nparameter estimation without requiring prior knowledge of nonlinear\ncharacteristics such as trigonometric frequencies, exponential bandwidths, or\npolynomial exponents, thereby addressing a key limitation of SINDy. Through an\nintegrated global optimization, ADAM-SINDy dynamically adjusts all unknown\nvariables in response to data, resulting in an adaptive identification\nprocedure that reduces the sensitivity to the library of candidate functions.\nThe performance of the ADAM-SINDy methodology is demonstrated across a spectrum\nof dynamical systems, including benchmark coupled nonlinear ordinary\ndifferential equations such as oscillators, chaotic fluid flows, reaction\nkinetics, pharmacokinetics, as well as nonlinear partial differential equations\n(wildfire transport). The results demonstrate significant improvements in\nidentifying parameterized dynamical systems and underscore the importance of\nconcurrently optimizing all parameters, particularly those characterized by\nnonlinear parameters. These findings highlight the potential of ADAM-SINDy to\nextend the applicability of the SINDy framework in addressing more complex\nchallenges in dynamical system identification.",
            "link": "http://arxiv.org/pdf/2410.16528v2"
        },
        {
            "title": "Generalization of the Painlevé Property and Existence and Uniqueness in Fractional Differential Equations",
            "year": "November 2024",
            "date": "2024-11-28",
            "authors": "Michał Fiedorowicz",
            "abstract": "In this paper, the Painlev\\'e property to fractional differential equations\n(FDEs) are extended and the existence and uniqueness theorems for both linear\nand nonlinear FDEs are established. The results contribute to the research of\nintegrability and solvability in the context of fractional calculus, which has\nsignificant implications in various fields such as physics, engineering, and\napplied sciences. By bridging the gap between pure mathematical theory and\npractical applications, this work provides a foundational understanding that\ncan be utilized in modeling phenomena exhibiting memory and hereditary\nproperties.",
            "link": "http://arxiv.org/pdf/2411.19411v1"
        },
        {
            "title": "ParallelFlow: Parallelizing Linear Transformers via Flow Discretization",
            "year": "April 2025",
            "date": "2025-04-01",
            "authors": "Nicola Muca Cirone, Cristopher Salvi",
            "abstract": "We present a theoretical framework for analyzing linear attention models\nthrough matrix-valued state space models (SSMs). Our approach, Parallel Flows,\nprovides a perspective that systematically decouples temporal dynamics from\nimplementation constraints, enabling independent analysis of critical\nalgorithmic components: chunking, parallelization, and information aggregation.\nCentral to this framework is the reinterpretation of chunking procedures as\ncomputations of the flows governing system dynamics. This connection\nestablishes a bridge to mathematical tools from rough path theory, opening the\ndoor to new insights into sequence modeling architectures. As a concrete\napplication, we analyze DeltaNet in a generalized low-rank setting motivated by\nrecent theoretical advances. Our methods allow us to design simple, streamlined\ngeneralizations of hardware-efficient algorithms present in the literature, and\nto provide completely different ones, inspired by rough paths techniques, with\nprovably lower complexity. This dual contribution demonstrates how principled\ntheoretical analysis can both explain existing practical methods and inspire\nfundamentally new computational approaches.",
            "link": "http://arxiv.org/pdf/2504.00492v1"
        },
        {
            "title": "Clustering in pure-attention hardmax transformers and its role in sentiment analysis",
            "year": "June 2024",
            "date": "2024-06-26",
            "authors": "Albert Alcalde, Giovanni Fantuzzi, Enrique Zuazua",
            "abstract": "Transformers are extremely successful machine learning models whose\nmathematical properties remain poorly understood. Here, we rigorously\ncharacterize the behavior of transformers with hardmax self-attention and\nnormalization sublayers as the number of layers tends to infinity. By viewing\nsuch transformers as discrete-time dynamical systems describing the evolution\nof points in a Euclidean space, and thanks to a geometric interpretation of the\nself-attention mechanism based on hyperplane separation, we show that the\ntransformer inputs asymptotically converge to a clustered equilibrium\ndetermined by special points called leaders. We then leverage this theoretical\nunderstanding to solve sentiment analysis problems from language processing\nusing a fully interpretable transformer model, which effectively captures\n`context' by clustering meaningless words around leader words carrying the most\nmeaning. Finally, we outline remaining challenges to bridge the gap between the\nmathematical analysis of transformers and their real-life implementation.",
            "link": "http://arxiv.org/pdf/2407.01602v1"
        }
    ],
    "last_5_years": [
        {
            "title": "A mathematical perspective on Transformers",
            "year": "December 2023",
            "date": "2023-12-17",
            "authors": "Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, Philippe Rigollet",
            "abstract": "Transformers play a central role in the inner workings of large language\nmodels. We develop a mathematical framework for analyzing Transformers based on\ntheir interpretation as interacting particle systems, which reveals that\nclusters emerge in long time. Our study explores the underlying theory and\noffers new perspectives for mathematicians as well as computer scientists.",
            "link": "http://arxiv.org/pdf/2312.10794v4"
        },
        {
            "title": "Coupled and Uncoupled Dynamic Mode Decomposition in Multi-Compartmental Systems with Applications to Epidemiological and Additive Manufacturing Problems",
            "year": "October 2021",
            "date": "2021-10-12",
            "authors": "Alex Viguerie, Gabriel F. Barros, Malú Grave, Alessandro Reali, Alvaro L. G. A. Coutinho",
            "abstract": "Dynamic Mode Decomposition (DMD) is an unsupervised machine learning method\nthat has attracted considerable attention in recent years owing to its\nequation-free structure, ability to easily identify coherent spatio-temporal\nstructures in data, and effectiveness in providing reasonably accurate\npredictions for certain problems. Despite these successes, the application of\nDMD to certain problems featuring highly nonlinear transient dynamics remains\nchallenging. In such cases, DMD may not only fail to provide acceptable\npredictions but may indeed fail to recreate the data in which it was trained,\nrestricting its application to diagnostic purposes. For many problems in the\nbiological and physical sciences, the structure of the system obeys a\ncompartmental framework, in which the transfer of mass within the system moves\nwithin states. In these cases, the behavior of the system may not be accurately\nrecreated by applying DMD to a single quantity within the system, as proper\nknowledge of the system dynamics, even for a single compartment, requires that\nthe behavior of other compartments is taken into account in the DMD process. In\nthis work, we demonstrate, theoretically and numerically, that, when performing\nDMD on a fully coupled PDE system with compartmental structure, one may recover\nuseful predictive behavior, even when DMD performs poorly when acting\ncompartment-wise. We also establish that important physical quantities, as mass\nconservation, are maintained in the coupled-DMD extrapolation. The mathematical\nand numerical analysis suggests that DMD may be a powerful tool when applied to\nthis common class of problems. In particular, we show interesting numerical\napplications to a continuous delayed-SIRD model for Covid-19, and to a problem\nfrom additive manufacturing considering a nonlinear temperature field and the\nresulting change of material phase from powder, liquid, and solid states.",
            "link": "http://arxiv.org/pdf/2110.06375v1"
        },
        {
            "title": "Some open problems in low dimensional dynamical systems",
            "year": "December 2020",
            "date": "2020-12-04",
            "authors": "Armengol Gasull",
            "abstract": "The aim of this paper is to share with the mathematical community a list of\n33 problems that I have found along the years during my research. I believe\nthat it is worth to think about them and, hopefully, it will be possible either\nto solve some of the problems or to make some substantial progress. Many of\nthem are about planar differential equations but there are also questions about\nother mathematical aspects: Abel differential equations, difference equations,\nglobal asymptotic stability, geometrical questions, problems involving\npolynomials or some recreational problems with a dynamical component.",
            "link": "http://arxiv.org/pdf/2012.02524v1"
        },
        {
            "title": "On Neural Differential Equations",
            "year": "February 2022",
            "date": "2022-02-04",
            "authors": "Patrick Kidger",
            "abstract": "The conjoining of dynamical systems and deep learning has become a topic of\ngreat interest. In particular, neural differential equations (NDEs) demonstrate\nthat neural networks and differential equation are two sides of the same coin.\nTraditional parameterised differential equations are a special case. Many\npopular neural network architectures, such as residual networks and recurrent\nnetworks, are discretisations.\n  NDEs are suitable for tackling generative problems, dynamical systems, and\ntime series (particularly in physics, finance, ...) and are thus of interest to\nboth modern machine learning and traditional mathematical modelling. NDEs offer\nhigh-capacity function approximation, strong priors on model space, the ability\nto handle irregular data, memory efficiency, and a wealth of available theory\non both sides.\n  This doctoral thesis provides an in-depth survey of the field.\n  Topics include: neural ordinary differential equations (e.g. for hybrid\nneural/mechanistic modelling of physical systems); neural controlled\ndifferential equations (e.g. for learning functions of irregular time series);\nand neural stochastic differential equations (e.g. to produce generative models\ncapable of representing complex stochastic dynamics, or sampling from complex\nhigh-dimensional distributions).\n  Further topics include: numerical methods for NDEs (e.g. reversible\ndifferential equations solvers, backpropagation through differential equations,\nBrownian reconstruction); symbolic regression for dynamical systems (e.g. via\nregularised evolution); and deep implicit models (e.g. deep equilibrium models,\ndifferentiable optimisation).\n  We anticipate this thesis will be of interest to anyone interested in the\nmarriage of deep learning with dynamical systems, and hope it will provide a\nuseful reference for the current state of the art.",
            "link": "http://arxiv.org/pdf/2202.02435v1"
        },
        {
            "title": "Scalable algorithms for physics-informed neural and graph networks",
            "year": "May 2022",
            "date": "2022-05-16",
            "authors": "Khemraj Shukla, Mengjia Xu, Nathaniel Trask, George Em Karniadakis",
            "abstract": "Physics-informed machine learning (PIML) has emerged as a promising new\napproach for simulating complex physical and biological systems that are\ngoverned by complex multiscale processes for which some data are also\navailable. In some instances, the objective is to discover part of the hidden\nphysics from the available data, and PIML has been shown to be particularly\neffective for such problems for which conventional methods may fail. Unlike\ncommercial machine learning where training of deep neural networks requires big\ndata, in PIML big data are not available. Instead, we can train such networks\nfrom additional information obtained by employing the physical laws and\nevaluating them at random points in the space-time domain. Such\nphysics-informed machine learning integrates multimodality and multifidelity\ndata with mathematical models, and implements them using neural networks or\ngraph networks. Here, we review some of the prevailing trends in embedding\nphysics into machine learning, using physics-informed neural networks (PINNs)\nbased primarily on feed-forward neural networks and automatic differentiation.\nFor more complex systems or systems of systems and unstructured data, graph\nneural networks (GNNs) present some distinct advantages, and here we review how\nphysics-informed learning can be accomplished with GNNs based on graph exterior\ncalculus to construct differential operators; we refer to these architectures\nas physics-informed graph networks (PIGNs). We present representative examples\nfor both forward and inverse problems and discuss what advances are needed to\nscale up PINNs, PIGNs and more broadly GNNs for large-scale engineering\nproblems.",
            "link": "http://arxiv.org/pdf/2205.08332v1"
        }
    ],
    "all_time": [
        {
            "title": "A Model of Blood Flow in a Circulation Network",
            "year": "October 2002",
            "date": "2002-10-26",
            "authors": "Weihua Ruan, M. E. Clark, Meide Zhao, Anthony Curcio",
            "abstract": "We study a mathematical model of a blood circulation network which is a\ngeneralization of the coronary model proposed by Smith, Pullan and Hunter. We\nprove the existence and uniqueness of the solution to the initial-boundary\nvalue problem and discuss the continuity of dependence of the solution and its\nderivatives on initial, boundary and forcing functions and their derivatives.",
            "link": "http://arxiv.org/pdf/math/0210410v1"
        },
        {
            "title": "Modeling the influence of TH1 and TH2 type cells in autoimmune diseases",
            "year": "June 2000",
            "date": "2000-06-19",
            "authors": "Y. Louzoun, H. Atlan, I. R. Cohen",
            "abstract": "A sharp TH1/TH2 dichotomy has often been used to define the effects of\ncytokines on autoimmune diseases. However contradictory results in recent\nresearch indicate that the situation may be more complex. We build here a\nsimple mathematical model aimed at settling the contradictions. The model is\nbased on a neural network paradigm, and is applied using Partial Differential\nEquations (PDE). We show here that a TH1/TH2 paradigm is only an external view\nof a complex multivariate system.",
            "link": "http://arxiv.org/pdf/math/0006127v1"
        },
        {
            "title": "Gain-induced oscillations in blood pressure",
            "year": "August 1997",
            "date": "1997-08-16",
            "authors": "Roselyn M. Abbiw-Jackson, William Langford",
            "abstract": "\"Mayer waves\" are long-period (6 to 12 seconds) oscillations in arterial\nblood pressure, which have been observed and studied for more than 100 years in\nthe cardiovascular system of humans and other mammals. A mathematical model of\nthe human cardiovascular system is presented, incorporating parameters\nrelevantto the onset of Mayer waves. The model is analyzed using methods of\nLyapunov stability and Hopf bifurcation theory. The analysis shows that\nincrease in the gain of the baroreflex feedback loop controlling venous volume\nmay lead to the onset of oscillations, while changes in the other parameters\nconsidered do not affect stability of the equilibrium state. The results agree\nwith clinical observations of Mayer waves in human subjects, both in the period\nof the oscillations and in the observed age-dependence of Mayer waves. This\nleads to a proposed explanation of their occurrence, namely that Mayer waves\nare a \"gain-induced instability\".",
            "link": "http://arxiv.org/pdf/math/9708211v1"
        },
        {
            "title": "Rhythms of the nervous system: mathematical themes and variations",
            "year": "May 2003",
            "date": "2003-05-01",
            "authors": "Nancy Kopell",
            "abstract": "The nervous system displays a variety of rhythms in both waking and sleep.\nThese rhythms have been closely associated with different behavioral and\ncognitive states, but it is still unknown how the nervous system makes use of\nthese rhythms to perform functionally important tasks. To address those\nquestions, it is first useful to understood in a mechanistic way the origin of\nthe rhythms, their interactions, the signals which create the transitions among\nrhythms, and the ways in which rhythms filter the signals to a network of\nneurons. This talk discusses how dynamical systems have been used to\ninvestigate the origin, properties and interactions of rhythms in the nervous\nsystem. It focuses on how the underlying physiology of the cells and synapses\nof the networks shape the dynamics of the network in different contexts,\nallowing the variety of dynamical behaviors to be displayed by the same\nnetwork. The work is presented using a series of related case studies on\ndifferent rhythms. These case studies are chosen to highlight mathematical\nissues, and suggest further mathematical work to be done. The topics include:\ndifferent roles of excitation and inhibition in creating synchronous assemblies\nof cells, different kinds of building blocks for neural oscillations, and\ntransitions among rhythms. The mathematical issues include reduction of large\nnetworks to low dimensional maps, role of noise, global bifurcations, use of\nprobabilistic formulations.",
            "link": "http://arxiv.org/pdf/math/0305013v1"
        },
        {
            "title": "Destruction of CD4 T Lymphocytes Alone Cannot Account for their Long-term Decrease in AIDS",
            "year": "August 2000",
            "date": "2000-08-07",
            "authors": "Yoram Louzoun, Irun. R. Cohen, Henri Atlan",
            "abstract": "Following previous models describing a quasi steady state (QSS) for the\nevolution of HIV infection and AIDS, we have developed a larger formalism\nsimulating the long-term evolution of the QSS We show that the long-term\nevolution of AIDS cannot be explained by the destruction alone of CD4 T cells,\neither directly or indirectly. The destruction of CD4 T cells can lead only to\na QSS with a lower concentration of CD4 T cells, but CD4 destruction cannot\ngenerate the sustained long-term decrease in T cells leading to AIDS. We here\nsuggest some workable explanations.",
            "link": "http://arxiv.org/pdf/math/0008052v1"
        }
    ]
}