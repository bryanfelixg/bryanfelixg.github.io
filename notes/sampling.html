<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Sampling</title>

  <!-- Style Sheet & Font-->
  <link rel="stylesheet" href="../css/stylecode.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&display=swap" rel="stylesheet">
  
  <!-- Navigation Sidebar Script -->
  <script defer src="../jss/navigation.js"></script>

  <!-- SageMath Library -->
  <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
  <script>
    // Make *any* div with class 'sage' a Sage cell
    sagecell.makeSagecell({inputLocation: 'div.sage',
                           evalButtonText: 'Run',
                           editor: "codemirror",
                           languages: ['python'],
                           linked: 'true'});
  </script>

  <!-- MathJax for LaTeX -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<div class="container"> 
<div class="sidebar">
<ul></ul> 
</div>
    
<article id="post-content">  
<header>
<h1 id="title"> Sampling Methods </h1>
<p class="author-subtitle">Bryan F√©lix</p> 
<time datetime="2024-08-15">Aug 19, 2024</time>
</header>

<h2 id="intro">Introduction</h2>

<p>This is a quick introduction to sampling methods for machine learning applications. The structure of the notebook is as follows. First, we create an artificial dataset to model. Then we will create a <em>polynomial regression</em> model to use as a template with an arbitrary degree. Each of the following sampling methods will be implemented in Python: <em>random sampling</em>, <em>k-fold cross-validation</em>, and <em>bootstrapping</em>. For each, we will compute the <em>test error rate</em> and compare the variance of the results.</p>

<h2 id="data">Simulating some data</h2>
<p>For exposition, let's simulate data points that <em>"look"</em> cubic. Let's take a few points in the interval \(x \in [-3, 3]\) and map them to the function \[f(x) = x^3 - 2x^2 + x + 3 + \varepsilon. \]
Here \(\varepsilon\) is random noise sampled from a normal distribution with mean (\(\mu\)) zero, and standard deviation (\(\sigma\)) equal to 2.</p>

<div class="sage">
<script type="text/sage">
import numpy as np
import matplotlib.pyplot as plt

ùúá = 0 
ùúé = 2
ùúÄ = np.random.normal(loc = ùúá, scale = ùúé, size = 100)

x = np.linspace(-3,3,100)
y = x**3 - 2*x**2 + x + 3 + ùúÄ

print(f"x: {x[:5]}...")
print(f"y: {y[:5]}...")

fig, ax = plt.subplots()
ax.scatter(x, y)
ax.plot(x,x**3 - 2*x**2 + x + 3, c='tab:orange')
ax.set_title("Simulated Data")
ax.set_xlabel("x")
ax.set_ylabel(r"$f(x) = x^3 - 2x^2 + x + 3$")
plt.show()
plt.close()
</script>
</div>

<p>Note that in approximating the <em>true</em> function \(f(x)\), a polynomial model of degree three will have an expected <em>mean squared error</em> equal to the variance of \(\varepsilon\). In other words, we expect an error approximately equal to \(\mathrm{Var}(\varepsilon) = \sigma^2 = 4\). Keep this in mind when looking at the estimates below.</p>

<h2 id="model">Polynomial Regression with Sci-Kit Learn</h2>

<p>While we know that the data is coming from a cubic polynomial with added noise, in a real-world context, we do not expect to have this insight. In fact, most of the time, real-world data does not resemble any particular polynomial to the naked eye. However, we can rationalize the choice of a polynomial model by examining error estimates from different models.</p>

<p>Therefore, let's assume we do <strong>not</strong> know anything about the degree of the data. Let's build a general linear polynomial model that allows us to use varying degrees. The method here is described in more detail at <a href="https://www.geeksforgeeks.org/python-implementation-of-polynomial-regression/">Geeks for Geeks</a>. The intuition is to <em>"engineer"</em> the features \(1, x, x^2, x^3, \dots, x^d \) for each data point \(x,y\) and then fit a <em>linear regression model</em> to these. Here we are calling \(d\) the degree of the polynomial. In technical notation, we are looking for coefficients \(\beta_0, \beta_1, \beta_2, \beta_3, ..., \beta_d\) that <strong>best</strong> approximate the solution for the following equation:</p>

\[
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix} 
=
\begin{bmatrix}
1 & x_{1} & x_{1}^2 & x_{1}^3 & \cdots & x_{1}^d \\
1 & x_{2} & x_{2}^2 & x_{2}^3 & \cdots & x_{2}^d \\
\vdots & \vdots & \vdots & \vdots \\
1 & x_{n} & x_{n}^2 & x_{n}^3 & \cdots & x_{n}^d
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3 \\
\vdots \\
\beta_d 
\end{bmatrix}
.
\]

<p>Here we call \(n\) the number of points in our dataset.</p>

<p>Below is the code implementation for the polynomial regression model and the estimate of the MSE. First, the function <code>polynomial_model</code> creates a polynomial model based on the given degree and the training data. Note that the output returns a tuple <code>(model, poly)</code>. The former is the fitted model, and the latter is the Scikit-learn polynomial "object" that allows us to use the feature engineering for our validation step.</p>
<p>Second, the <code>get_test_error</code> function takes both outputs from the first function and the test dataset to compute the <em>mean squared error</em> of the model prediction.</p>

<p>Note that the choice of <em>mean squared error</em> as a benchmark is rather <em>traditional</em> than objective. I would encourage anyone to explore more <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">built-in metrics</a> in Scikit-learn.</p>

<div class="sage">
<script type="text/sage">
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

def polynomial_model(degree, x_train, y_train):
  x_train = x_train.reshape(-1,1)
  poly = PolynomialFeatures(degree = degree)
  x_poly = poly.fit_transform(x_train)
  model = LinearRegression()
  model.fit(x_poly, y_train)
  return model, poly

def get_test_error(model, poly, x_test, y_test):
  x_test = x_test.reshape(-1,1)
  x_test_poly = poly.transform(x_test)
  y_pred = model.predict(x_test_poly)
  mse = mean_squared_error(y_test, y_pred)
  return mse    
</script>
</div>

<h2 id="rand_split">Random Splitting</h2>

<p>With the code above, we can now start inspecting different methods for estimating the <em>test error</em> as defined in the textbook <em><a href="https://www.statlearning.com">An Introduction to Statistical Learning, with Applications</a> by James, et al.</em> Namely, what is the model's error on the space of <em>unseen</em> observations? In other words, how well does the model generalize predictions based on input not in the <em>training</em> dataset?</p>

<p>The first method here is <em>random splitting</em>. We take the whole dataset available and set aside a proportion of values that we will use to estimate the prediction error. In other texts, you may see this referred to as the <em>validation error</em>. Scikit-learn has a built-in function to randomly split the data. We will call this method and prescribe 1) the degree of the model we want to fit, 2) the proportion of data that will be used for testing, and 3) the number of times we will fit the model on random splits of the data.</p>

<p>Recall that for a third-degree polynomial fit, we would expect an error of \(4\). We'll use this benchmark to observe how the random splitting method might underestimate or overestimate the true error.</p>

<div class="sage">
<script type="text/sage">
from sklearn.model_selection import train_test_split

degree = 2
num_splits = 20
test_ratio = 0.2

split_errors = []
for _ in range(num_splits):
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio)
    model, poly = polynomial_model(degree, x_train, y_train)
    mse = get_test_error(model, poly, x_test, y_test)
    split_errors.append(mse)

# Plotting
fig, ax = plt.subplots()
ax.boxplot(split_errors)
ax.axhline(y=4, color='r', linestyle='--', label='Expected Error')
ax.set_xticks([])
ax.legend()
plt.ylabel("Test Error (MSE)")
plt.title(f"Test Error Rates for Polynomial Degree {degree} and Random Splits")
plt.show()
plt.close()

# Error Stats
print(f"Error Mean : {np.mean(split_errors)}")
print(f"Error Variance : {np.var(split_errors)}")
</script>
</div>

<p>The plots generated above help in understanding the error's behavior. However, it's important to note that this random splitting method tends to overestimate the true error, but not always. Additionally, the variance of this estimate can be high when compared to other methods like k-fold cross-validation. Nevertheless, this method is simple to implement and depending on the specifics of the dataset it may be computationally faster.</p>

<p>Lastly, it's worth highlighting how resampling can be used in fine-tuning hyperparameters and comparing different models. By estimating error rates for various model choices, and then comparing their error distributions, we may be able to identify more performant candidate models. The code snippet below demonstrates this process for polynomial regression models with varying degrees.</p>


<div class="sage">
<script type="text/sage">
degrees = range(1,11)
num_splits = 20
test_ratio = 0.2

error_rates = []
for degree in degrees:
    split_errors = []
    for _ in range(num_splits):
        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio)
        model, poly = polynomial_model(degree, x_train, y_train)
        mse = get_test_error(model, poly, x_test, y_test)
        split_errors.append(mse)
    error_rates.append(split_errors)


# Plotting
fig, ax = plt.subplots()
ax.boxplot(error_rates, labels=degrees)
ax.axhline(y=4, color='r', linestyle='--', label='Expected Error')
ax.legend()
plt.xlabel("Polynomial Degree")
plt.ylabel("Test Error (MSE)")
plt.title("Test Error Rates for Different Polynomial Degrees and Random Splits")
plt.show()
plt.close()
</script>
</div>

<p>In the plots adobe note that there are significant gains from polynomial degrees one to three. After the their degree polynomial the error estimate is stable and we only get marginal gains. However, the complexity of the model would be undesirable and higher degrees would introduce higher variance (bias-variance trade-off). Therefore, we would prefer to use the degree three polynomial. Of course, we already know that the dataset was cubic from the start. But it is reassuring to see that the theory supports this choice. Lastly, let us note that in general the <em>random sampling</em> approach tends to <strong>overestimate</strong> the error.</p>

<h2 id=k_fold>The \(k\)-fold Cross-Validation Method</h2>

<p>We will now see a different approach for estimating the error, namely <em>\(\mathbf{k}\)-fold cross-validation</em>. This method differentiates from random sampling in that the validation and training sets are split in a more consistent way. The diagram below describes the process at a high level. First, we chose \(k\) to create \(k\) subsets of the original dataset. We will train the model \(k\) times by using one subset as the validation set and the remaining \(k-1\) subsets as the training set.</p>

<div class="image-container">
  <img src="images/kfold.png" alt="k-fold">
  <figcaption>\(k\)-Fold Cross-Validation.Taken from <a href="https://towardsdatascience.com/cross-validation-k-fold-vs-monte-carlo-e54df2fc179b">Towards Data Science</a></figcaption> 
</div>

<p>Generally, the value \(k\) should be chosen proportional to the computational cost of training the model since the model will be trained \(k\) different times. Some texts suggest using \(k\) between 5 and 8. We implement this idea with our running example. The code is taken but simplified from this example in <a href="https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/">Geeks for Geeks.</a></p>

<div class="sage">
<script type="text/sage">
from sklearn.model_selection import KFold

degree = 2
num_folds = 10
kf = KFold(n_splits=num_folds, shuffle=True)

fold_errors = []
for train_index, test_index in kf.split(x):
    x_train, x_test = x[train_index], x[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model, poly = polynomial_model(degree, x_train, y_train)
    mse = get_test_error(model, poly, x_test, y_test)
    fold_errors.append(mse)

# Plotting
fig, ax = plt.subplots()
ax.boxplot(fold_errors)
ax.axhline(y=4, color='r', linestyle='--', label='Expected Error')
ax.set_xticks([])
ax.legend()
plt.ylabel("Test Error (MSE)")
plt.title(f"Test Error Rates for Polynomial Degree {degree} and K-Fold CV")
plt.show()
plt.close()

# Error Stats
print(f"Error Mean : {np.mean(fold_errors)}")
print(f"Error Variance : {np.var(fold_errors)}")
</script>
</div>

<p>Note in the implementation above that the error estimate is much closer to the actual error. Moreover, the variance of reduces significantly. More than that, if you pay close attention to the default values of the code implementation, you can see that whereas random sampling used 20 samples, we only have to use 10 folds for a better estimate. Still, we can often expect the \(k\)-fold estimate to <strong>underestimate</strong> the true error. Another downside of the method is that it is a jump in complexity from random splitting. Nevertheless it is worth learning. </p>

<p>Just as with random splitting, we can use \(k\)-fold cross validation to inspect the performance of different models. The code below is a similar implementation to the one used in random sampling. The conclusions are just the same, but we highlight again that the estimates have a lower variance across the board.</p>

<div class="sage">
<script type="text/sage">
from sklearn.model_selection import KFold

degrees = range(1,11)
num_folds = 5

kf = KFold(n_splits=num_folds, shuffle=True)

error_rates = []
for degree in degrees:
    fold_errors = []
    for train_index, test_index in kf.split(x):
        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model, poly = polynomial_model(degree, x_train, y_train)
        mse = get_test_error(model, poly, x_test, y_test)
        fold_errors.append(mse)
    error_rates.append(fold_errors)

# Plotting
fig, ax = plt.subplots()
ax.boxplot(error_rates, labels=degrees)
ax.axhline(y=4, color='r', linestyle='--', label='Expected Error')
ax.legend()
plt.xlabel("Polynomial Degree")
plt.ylabel("Test Error (MSE)")
plt.title("Test Error Rates for Different Polynomial Degrees using K-Fold Cross-Validation")
plt.show()
plt.close()
</script>
</div>

<h2 id="boot">Bootstrapping</h2>

<p>The final sampling method we'll discuss is Bootstrapping. The core principle of bootstrapping is to resample data from the original dataset with replacement. While Scikit-Learn doesn't have a dedicated bootstrapping implementation, we can create bootstrap samples using the <code>resample</code> method from <code>sklearn.utils</code>. </p>

<div class="sage">
<script type="text/sage">
from sklearn.utils import resample

degree = 2
num_bootstrap_samples = 20

bootstrap_errors = []
all_indices = range(len(x))
for _ in range(num_bootstrap_samples):
    # Create a bootstrap sample from indices with replacement
    bootstrap_indices = resample(all_indices, replace=True)
    x_train, y_train = x[bootstrap_indices], y[bootstrap_indices]

    test_indices = list(set(all_indices) - set(bootstrap_indices))
    x_test, y_test = x[test_indices], y[test_indices]

    model, poly = polynomial_model(degree, x_train, y_train) 
    mse = get_test_error(model, poly, x_test, y_test)
    bootstrap_errors.append(mse)

# Plotting
fig, ax = plt.subplots()
ax.boxplot(bootstrap_errors)
ax.axhline(y=4, color='r', linestyle='--', label='Expected Error')
ax.set_xticks([])
ax.legend()
plt.ylabel("Test Error (MSE)")
plt.title(f"Test Error Rates for Polynomial Degree {degree} and Bootstrapping")
plt.show()
plt.close()

# Error Stats
print(f"Error Mean : {np.mean(bootstrap_errors)}")
print(f"Error Variance : {np.var(bootstrap_errors)}")
</script>
</div>

<p>And as before, we can compare models with this method. The conclusions are analogous.</p>

<div class="sage">
<script type="text/sage">
from sklearn.utils import resample

degrees = range(1, 11)
num_bootstrap_samples = 100

error_rates = []
for degree in degrees:
    bootstrap_errors = []
    for _ in range(num_bootstrap_samples):
        # Create a bootstrap sample
        x_train, y_train = resample(x, y, replace=True)
        train_indices = set(range(len(x))) - set(x_train[:])
        x_test = x[list(train_indices)]
        y_test = y[list(train_indices)]

        model, poly = polynomial_model(degree, x_train, y_train) 
        mse = get_test_error(model, poly, x_test, y_test)
        bootstrap_errors.append(mse)
    error_rates.append(bootstrap_errors)

# Plotting
fig, ax = plt.subplots()
ax.boxplot(error_rates, labels=degrees)
plt.xlabel("Polynomial Degree")
plt.ylabel("Test Error (MSE)")
plt.title("Test Error Rates for Different Polynomial Degrees using Bootstrapping")
plt.show()
plt.close()
</script>
</div>

</article>
</div>
</body>
</html>
