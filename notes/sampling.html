<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Sampling</title>

  <!-- Style Sheet & Font-->
  <link rel="stylesheet" href="../css/stylecode.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&display=swap" rel="stylesheet">
  
  <!-- Navigation Sidebar Script -->
  <script defer src="../jss/navigation.js"></script>

  <!-- SageMath Library -->
  <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
  <script>
    // Make *any* div with class 'sage' a Sage cell
    sagecell.makeSagecell({inputLocation: 'div.sage',
                           evalButtonText: 'Run',
                           editor: "codemirror",
                           languages: ['python'],
                           linked: 'true'});
  </script>

  <!-- MathJax for LaTeX -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<div class="container"> 
<div class="sidebar">
<ul></ul> 
</div>
    
<article id="post-content">  
<header>
<h1 id="title"> Sampling Methods </h1>
<p class="author-subtitle">Bryan F√©lix</p> 
<time datetime="2024-08-15">Aug 19, 2024</time>
</header>

<h2 id="intro">Introduction</h2>
<p>This is a quick introduction to sampling methods for machine learning applications. The structure of the notebook is as follows. First we create an artificial set of data to model. Then we will create a <em>polynomial regression</em> model to use as template with arbitrary degree. Each of the following sampling methods will be implemented in python: <em>random sampling</em>, <em> \(k-\)fold cross-validation</em>, and, <em>bootstraping</em>. For each we will compute the <em>test error rate</em> and compare the variance of the results.</p>

<h2 id="data">Simulating some data</h2>
<p>For exposition, lets simulate data points that <em>"look"</em> cubic. Lets take a few points in the interval \(x \in [-3,-3]\) and map them to the function \[f(x) = x^3 -2x^2 +x +3 + \varepsilon. \]
Where \(\varepsilon\) is random noise sampled from a normal distribution with mean (\(\mu\)) zero, and standard deviation (\(\sigma\)) equal to 2. 
</p>

<div class="sage">
<script type="text/sage">
import numpy as np
ùúá = 0 
ùúé = 2
ùúÄ = np.random.normal(loc = ùúá, scale = ùúé, size = 100)

x = np.linspace(-3,3,100)
y = x**3 - 2*x**2 + x + 3 + ùúÄ

print(f"x: {x[:3]}...\ny: {y[:3]}...")
</script>
</div>
<br>
<div class="sage">
<script type="text/sage">
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_title("Simulated Data")
ax.set_xlabel("x")
ax.set_ylabel("f(x) = x^3 - 2x^2 + x + 3")
plt.show()
</script>
</div>

<h2 id="model">Polynomial Regression with Sci-Kit Learn</h2>
<p>Lets assume we do <strong>not</strong> know the degree of the data <em>a priori</em>. Therefore, lets build a general linear polynomial model that allows us to use varying degrees. The method here is described in more detail at <a href="https://www.geeksforgeeks.org/python-implementation-of-polynomial-regression/">Geeks for Geeks</a>. However, the intuition is to <em>"engineer"</em> the features \(1, x, x^2, x^3\) for each value \(x\) and then fit a <em>linear regression model</em> into the new features. In technical notation, we are looking for coefficients \(\beta_0,\beta_1, \beta_2, \beta_3\) that <strong>best</strong> approximate the solution for the following equation: \[
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix} 
=
\begin{bmatrix}
1 & x_{1} & x_{1}^2 & x_{1}^3 \\
1 & x_{2} & x_{2}^2 & x_{2}^3 \\
\vdots & \vdots & \vdots & \vdots \\
1 & x_{n} & x_{n}^2 & x_{n}^3
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3
\end{bmatrix}
.\]
Where \(x_i\) is the data point with value \(y_i\).</p>

<div class="sage">
<script type="text/sage">
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

def polynomial_model(degree, x_train, y_train):
  x_train = x_train.reshape(-1,1)
  poly = PolynomialFeatures(degree = degree)
  x_poly = poly.fit_transform(x_train)
  model = LinearRegression()
  model.fit(x_poly, y_train)
  return model, poly
</script>
</div>

<div class="sage">
<script type="text/sage">
from sklearn.metrics import mean_squared_error
def get_test_error(model, poly, x_test, y_test):
  x_test = x_test.reshape(-1,1)
  x_test_poly = poly.transform(x_test)
  y_pred = model.predict(x_test_poly)
  mse = mean_squared_error(y_test, y_pred)
  return mse    
</script>
</div>

<h2 id="rand_split">Random Splitting</h2>

<div class="sage">
<script type="text/sage">
from sklearn.model_selection import train_test_split

degree = 2
num_splits = 10
ratio = 0.5

split_errors = []
for _ in range(num_splits):
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=ratio)
    model, poly = polynomial_model(degree, x_train, y_train)
    mse = get_test_error(model, poly, x_test, y_test)
    split_errors.append(mse)

fig, ax = plt.subplots()
ax.boxplot(split_errors, labels=f"{degree}")
plt.xlabel("Polynomial Degree")
plt.ylabel("Test Error (MSE)")
plt.title(f"Test Error Rates for Polynomial Degree {degree} and Random Splits")
plt.show()
</script>
</div>

Testing Multiple Polynomial Models

<div class="sage">
<script type="text/sage">
degrees = range(1,11)
num_splits = 100

error_rates = []
for degree in degrees:
    split_errors = []
    for _ in range(num_splits):
        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
        model, poly = polynomial_model(degree, x_train, y_train)
        mse = get_test_error(model, poly, x_test, y_test)
        split_errors.append(mse)
    error_rates.append(split_errors)


# Plotting
fig, ax = plt.subplots()
ax.boxplot(error_rates, labels=degrees)
plt.xlabel("Polynomial Degree")
plt.ylabel("Test Error (MSE)")
plt.title("Test Error Rates for Different Polynomial Degrees and Random Splits")
plt.show()
</script>
</div>


<h2 id:k_fold>The \(k\)-fold Cross-Validation Method</h2>

<div class="sage">
<script type="text/sage">
from sklearn.model_selection import KFold
degrees = range(1,11)
num_folds = 5
kf = KFold(n_splits=num_folds, shuffle=True)

error_rates = []
for degree in degrees:
    fold_errors = []
    for train_index, test_index in kf.split(x):
        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model, poly = polynomial_model(degree, x_train, y_train)
        mse = get_test_error(model, poly, x_test, y_test)
        fold_errors.append(mse)
    error_rates.append(fold_errors)

# Plotting
fig, ax = plt.subplots()
ax.boxplot(error_rates, labels=degrees)
plt.xlabel("Polynomial Degree")
plt.ylabel("Test Error (MSE)")
plt.title("Test Error Rates for Different Polynomial Degrees using K-Fold Cross-Validation")
plt.show()

</script>
</div>

<h2 id="boot">Bootstraping</h2>

</article>

</div>

</body>
</html>
