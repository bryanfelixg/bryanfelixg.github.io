[
    {
        "title": "Evolutionary Dispersal of Ecological Species via Multi-Agent Deep Reinforcement Learning",
        "year": "October 2024",
        "date": "2024-10-24",
        "authors": "Wonhyung Choi, Inkyung Ahn",
        "abstract": "Understanding species dynamics in heterogeneous environments is essential for\necosystem studies. Traditional models assumed homogeneous habitats, but recent\napproaches include spatial and temporal variability, highlighting species\nmigration. We adopt starvation-driven diffusion (SDD) models as nonlinear\ndiffusion to describe species dispersal based on local resource conditions,\nshowing advantages for species survival. However, accurate prediction remains\nchallenging due to model simplifications. This study uses multi-agent\nreinforcement learning (MARL) with deep Q-networks (DQN) to simulate single\nspecies and predator-prey interactions, incorporating SDD-type rewards. Our\nsimulations reveal evolutionary dispersal strategies, providing insights into\nspecies dispersal mechanisms and validating traditional mathematical models.",
        "link": "http://arxiv.org/pdf/2410.18621v1"
    },
    {
        "title": "ADAM-SINDy: An Efficient Optimization Framework for Parameterized Nonlinear Dynamical System Identification",
        "year": "October 2024",
        "date": "2024-10-21",
        "authors": "Siva Viknesh, Younes Tatari, Amirhossein Arzani",
        "abstract": "Identifying dynamical systems characterized by nonlinear parameters presents\nsignificant challenges in deriving mathematical models that enhance\nunderstanding of physics. Traditional methods, such as Sparse Identification of\nNonlinear Dynamics (SINDy) and symbolic regression, can extract governing\nequations from observational data; however, they also come with distinct\nadvantages and disadvantages. This paper introduces a novel method within the\nSINDy framework, termed ADAM-SINDy, which synthesizes the strengths of\nestablished approaches by employing the ADAM optimization algorithm. This\nfacilitates the simultaneous optimization of nonlinear parameters and\ncoefficients associated with nonlinear candidate functions, enabling precise\nparameter estimation without requiring prior knowledge of nonlinear\ncharacteristics such as trigonometric frequencies, exponential bandwidths, or\npolynomial exponents, thereby addressing a key limitation of SINDy. Through an\nintegrated global optimization, ADAM-SINDy dynamically adjusts all unknown\nvariables in response to data, resulting in an adaptive identification\nprocedure that reduces the sensitivity to the library of candidate functions.\nThe performance of the ADAM-SINDy methodology is demonstrated across a spectrum\nof dynamical systems, including benchmark coupled nonlinear ordinary\ndifferential equations such as oscillators, chaotic fluid flows, reaction\nkinetics, pharmacokinetics, as well as nonlinear partial differential equations\n(wildfire transport). The results demonstrate significant improvements in\nidentifying parameterized dynamical systems and underscore the importance of\nconcurrently optimizing all parameters, particularly those characterized by\nnonlinear parameters. These findings highlight the potential of ADAM-SINDy to\nextend the applicability of the SINDy framework in addressing more complex\nchallenges in dynamical system identification.",
        "link": "http://arxiv.org/pdf/2410.16528v1"
    },
    {
        "title": "Learning the Rolling Penny Dynamics",
        "year": "October 2024",
        "date": "2024-10-19",
        "authors": "Baiyue Wang, Anthony Bloch",
        "abstract": "We consider learning the dynamics of a typical nonholonomic system -- the\nrolling penny. A nonholonomic system is a system subject to nonholonomic\nconstraints. Unlike holonomic constraints, a nonholonomic constraint does not\ndefine a submanifold on the configuration space. Therefore, the inverse problem\nof finding the constraints has to involve the tangent space. This paper discuss\nhow to learn the dynamics, as well as the constraints for such a system given\nthe data set of discrete trajectories on the tangent bundle \\(TQ\\).",
        "link": "http://arxiv.org/pdf/2410.15201v1"
    },
    {
        "title": "Data-driven identification of latent port-Hamiltonian systems",
        "year": "August 2024",
        "date": "2024-08-15",
        "authors": "Johannes Rettberg, Jonas Kneifl, Julius Herb, Patrick Buchfink, Jörg Fehr, Bernard Haasdonk",
        "abstract": "Conventional physics-based modeling techniques involve high effort, e.g.,\ntime and expert knowledge, while data-driven methods often lack\ninterpretability, structure, and sometimes reliability. To mitigate this, we\npresent a data-driven system identification framework that derives models in\nthe port-Hamiltonian (pH) formulation. This formulation is suitable for\nmulti-physical systems while guaranteeing the useful system theoretical\nproperties of passivity and stability. Our framework combines linear and\nnonlinear reduction with structured, physics-motivated system identification.\nIn this process, high-dimensional state data obtained from possibly nonlinear\nsystems serves as input for an autoencoder, which then performs two tasks: (i)\nnonlinearly transforming and (ii) reducing this data onto a low-dimensional\nlatent space. In this space, a linear pH system, that satisfies the pH\nproperties per construction, is parameterized by the weights of a neural\nnetwork. The mathematical requirements are met by defining the pH matrices\nthrough Cholesky factorizations. The neural networks that define the coordinate\ntransformation and the pH system are identified in a joint optimization process\nto match the dynamics observed in the data while defining a linear pH system in\nthe latent space. The learned, low-dimensional pH system can describe even\nnonlinear systems and is rapidly computable due to its small size. The method\nis exemplified by a parametric mass-spring-damper and a nonlinear pendulum\nexample, as well as the high-dimensional model of a disc brake with linear\nthermoelastic behavior.",
        "link": "http://arxiv.org/pdf/2408.08185v2"
    },
    {
        "title": "Clustering in pure-attention hardmax transformers and its role in sentiment analysis",
        "year": "June 2024",
        "date": "2024-06-26",
        "authors": "Albert Alcalde, Giovanni Fantuzzi, Enrique Zuazua",
        "abstract": "Transformers are extremely successful machine learning models whose\nmathematical properties remain poorly understood. Here, we rigorously\ncharacterize the behavior of transformers with hardmax self-attention and\nnormalization sublayers as the number of layers tends to infinity. By viewing\nsuch transformers as discrete-time dynamical systems describing the evolution\nof points in a Euclidean space, and thanks to a geometric interpretation of the\nself-attention mechanism based on hyperplane separation, we show that the\ntransformer inputs asymptotically converge to a clustered equilibrium\ndetermined by special points called leaders. We then leverage this theoretical\nunderstanding to solve sentiment analysis problems from language processing\nusing a fully interpretable transformer model, which effectively captures\n`context' by clustering meaningless words around leader words carrying the most\nmeaning. Finally, we outline remaining challenges to bridge the gap between the\nmathematical analysis of transformers and their real-life implementation.",
        "link": "http://arxiv.org/pdf/2407.01602v1"
    },
    {
        "title": "Reservoir Computing with Generalized Readout based on Generalized Synchronization",
        "year": "May 2024",
        "date": "2024-05-03",
        "authors": "Akane Ookubo, Masanobu Inubushi",
        "abstract": "Reservoir computing is a machine learning framework that exploits nonlinear\ndynamics, exhibiting significant computational capabilities. One of the\ndefining characteristics of reservoir computing is its low cost and\nstraightforward training algorithm, i.e. only the readout, given by a linear\ncombination of reservoir variables, is trained. Inspired by recent mathematical\nstudies based on dynamical system theory, in particular generalized\nsynchronization, we propose a novel reservoir computing framework with\ngeneralized readout, including a nonlinear combination of reservoir variables.\nThe first crucial advantage of using the generalized readout is its\nmathematical basis for improving information processing capabilities. Secondly,\nit is still within a linear learning framework, which preserves the original\nstrength of reservoir computing. In summary, the generalized readout is\nnaturally derived from mathematical theory and allows the extraction of useful\nbasis functions from reservoir dynamics without sacrificing simplicity. In a\nnumerical study, we find that introducing the generalized readout leads to a\nsignificant improvement in accuracy and an unexpected enhancement in robustness\nfor the short- and long-term prediction of Lorenz chaos, with a particular\nfocus on how to harness low-dimensional reservoir dynamics. A novel way and its\nadvantages for physical implementations of reservoir computing with generalized\nreadout are briefly discussed.",
        "link": "http://arxiv.org/pdf/2405.14885v1"
    },
    {
        "title": "Machine-learning invariant foliations in forced systems for reduced order modelling",
        "year": "March 2024",
        "date": "2024-03-21",
        "authors": "Robert Szalai",
        "abstract": "We identify reduced order models (ROM) of forced systems from data using\ninvariant foliations. The forcing can be external, parametric, periodic or\nquasi-periodic. The process has four steps: 1. identify an approximate\ninvariant torus and the linear dynamics about the torus; 2. identify a globally\ndefined invariant foliation about the torus; 3. identify a local foliation\nabout an invariant manifold that complements the global foliation 4. extract\nthe invariant manifold as the leaf going through the torus and interpret the\nresult. We combine steps 2 and 3, so that we can track the location of the\ninvariant torus and scale the invariance equations appropriately. We highlight\nsome fundamental limitations of invariant manifolds and foliations when fitting\nthem to data, that require further mathematics to resolve.",
        "link": "http://arxiv.org/pdf/2403.14514v1"
    },
    {
        "title": "The Impact of LoRA on the Emergence of Clusters in Transformers",
        "year": "February 2024",
        "date": "2024-02-23",
        "authors": "Hugo Koubbi, Matthieu Boussard, Louis Hernandez",
        "abstract": "In this paper, we employ the mathematical framework on Transformers developed\nby\n\\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical}\nto explore how variations in attention parameters and initial token values\nimpact the structural dynamics of token clusters. Our analysis demonstrates\nthat while the clusters within a modified attention matrix dynamics can exhibit\nsignificant divergence from the original over extended periods, they maintain\nclose similarities over shorter intervals, depending on the parameter\ndifferences. This work contributes to the fine-tuning field through practical\napplications to the LoRA algorithm \\cite{hu2021lora,peft}, enhancing our\nunderstanding of the behavior of LoRA-enhanced Transformer models.",
        "link": "http://arxiv.org/pdf/2402.15415v1"
    },
    {
        "title": "Differential Equations for Continuous-Time Deep Learning",
        "year": "January 2024",
        "date": "2024-01-08",
        "authors": "Lars Ruthotto",
        "abstract": "This short, self-contained article seeks to introduce and survey\ncontinuous-time deep learning approaches that are based on neural ordinary\ndifferential equations (neural ODEs). It primarily targets readers familiar\nwith ordinary and partial differential equations and their analysis who are\ncurious to see their role in machine learning. Using three examples from\nmachine learning and applied mathematics, we will see how neural ODEs can\nprovide new insights into deep learning and a foundation for more efficient\nalgorithms.",
        "link": "http://arxiv.org/pdf/2401.03965v1"
    },
    {
        "title": "A mathematical perspective on Transformers",
        "year": "December 2023",
        "date": "2023-12-17",
        "authors": "Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, Philippe Rigollet",
        "abstract": "Transformers play a central role in the inner workings of large language\nmodels. We develop a mathematical framework for analyzing Transformers based on\ntheir interpretation as interacting particle systems, which reveals that\nclusters emerge in long time. Our study explores the underlying theory and\noffers new perspectives for mathematicians as well as computer scientists.",
        "link": "http://arxiv.org/pdf/2312.10794v4"
    },
    {
        "title": "Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation, and Benchmarks",
        "year": "July 2023",
        "date": "2023-07-20",
        "authors": "Victor Churchill, Dongbin Xiu",
        "abstract": "Flow map learning (FML), in conjunction with deep neural networks (DNNs), has\nshown promises for data driven modeling of unknown dynamical systems. A\nremarkable feature of FML is that it is capable of producing accurate\npredictive models for partially observed systems, even when their exact\nmathematical models do not exist. In this paper, we present an overview of the\nFML framework, along with the important computational details for its\nsuccessful implementation. We also present a set of well defined benchmark\nproblems for learning unknown dynamical systems. All the numerical details of\nthese problems are presented, along with their FML results, to ensure that the\nproblems are accessible for cross-examination and the results are reproducible.",
        "link": "http://arxiv.org/pdf/2307.11013v1"
    },
    {
        "title": "On the convergence of nonlinear averaging dynamics with three-body interactions on hypergraphs",
        "year": "April 2023",
        "date": "2023-04-14",
        "authors": "Emilio Cruciani, Emanuela L. Giacomelli, Jinyeop Lee",
        "abstract": "Complex networked systems in fields such as physics, biology, and social\nsciences often involve interactions that extend beyond simple pairwise ones.\nHypergraphs serve as powerful modeling tools for describing and analyzing the\nintricate behaviors of systems with multi-body interactions. Herein, we\ninvestigate a discrete-time nonlinear averaging dynamics with three-body\ninteractions: an underlying hypergraph, comprising triples as hyperedges,\ndelineates the structure of these interactions, while the vertices update their\nstates through a weighted, state-dependent average of neighboring pairs'\nstates. This dynamics captures reinforcing group effects, such as peer\npressure, and exhibits higher-order dynamical effects resulting from a complex\ninterplay between initial states, hypergraph topology, and nonlinearity of the\nupdate. Differently from linear averaging dynamics on graphs with two-body\ninteractions, this model does not converge to the average of the initial states\nbut rather induces a shift. By assuming random initial states and by making\nsome regularity and density assumptions on the hypergraph, we prove that the\ndynamics converges to a multiplicatively-shifted average of the initial states,\nwith high probability. We further characterize the shift as a function of two\nparameters describing the initial state and interaction strength, as well as\nthe convergence time as a function of the hypergraph structure.",
        "link": "http://arxiv.org/pdf/2304.07203v2"
    },
    {
        "title": "Direct Estimation of Parameters in ODE Models Using WENDy: Weak-form Estimation of Nonlinear Dynamics",
        "year": "February 2023",
        "date": "2023-02-26",
        "authors": "David M. Bortz, Daniel A. Messenger, Vanja Dukic",
        "abstract": "We introduce the Weak-form Estimation of Nonlinear Dynamics (WENDy) method\nfor estimating model parameters for non-linear systems of ODEs. Without relying\non any numerical differential equation solvers, WENDy computes accurate\nestimates and is robust to large (biologically relevant) levels of measurement\nnoise. For low dimensional systems with modest amounts of data, WENDy is\ncompetitive with conventional forward solver-based nonlinear least squares\nmethods in terms of speed and accuracy. For both higher dimensional systems and\nstiff systems, WENDy is typically both faster (often by orders of magnitude)\nand more accurate than forward solver-based approaches.\n  The core mathematical idea involves an efficient conversion of the strong\nform representation of a model to its weak form, and then solving a regression\nproblem to perform parameter inference. The core statistical idea rests on the\nErrors-In-Variables framework, which necessitates the use of the iteratively\nreweighted least squares algorithm. Further improvements are obtained by using\northonormal test functions, created from a set of C-infinity bump functions of\nvarying support sizes.\n  We demonstrate the high robustness and computational efficiency by applying\nWENDy to estimate parameters in some common models from population biology,\nneuroscience, and biochemistry, including logistic growth, Lotka-Volterra,\nFitzHugh-Nagumo, Hindmarsh-Rose, and a Protein Transduction Benchmark model.\nSoftware and code for reproducing the examples is available at\n(https://github.com/MathBioCU/WENDy).",
        "link": "http://arxiv.org/pdf/2302.13271v3"
    },
    {
        "title": "The Lost Art of Mathematical Modelling",
        "year": "January 2023",
        "date": "2023-01-19",
        "authors": "Linnéa Gyllingberg, Abeba Birhane, David J. T. Sumpter",
        "abstract": "We provide a critique of mathematical biology in light of rapid developments\nin modern machine learning. We argue that out of the three modelling activities\n-- (1) formulating models; (2) analysing models; and (3) fitting or comparing\nmodels to data -- inherent to mathematical biology, researchers currently focus\ntoo much on activity (2) at the cost of (1). This trend, we propose, can be\nreversed by realising that any given biological phenomena can be modelled in an\ninfinite number of different ways, through the adoption of an open/pluralistic\napproach. We explain the open approach using fish locomotion as a case study\nand illustrate some of the pitfalls -- universalism, creating models of models,\netc. -- that hinder mathematical biology. We then ask how we might rediscover a\nlost art: that of creative mathematical modelling.\n  This article is dedicated to the memory of Edmund Crampin.",
        "link": "http://arxiv.org/pdf/2301.08559v2"
    },
    {
        "title": "Generalized Quadratic Embeddings for Nonlinear Dynamics using Deep Learning",
        "year": "November 2022",
        "date": "2022-11-01",
        "authors": "Pawan Goyal, Peter Benner",
        "abstract": "The engineering design process often relies on mathematical modeling that can\ndescribe the underlying dynamic behavior. In this work, we present a\ndata-driven methodology for modeling the dynamics of nonlinear systems. To\nsimplify this task, we aim to identify a coordinate transformation that allows\nus to represent the dynamics of nonlinear systems using a common, simple model\nstructure. The advantage of a common simple model is that customized design\ntools developed for it can be applied to study a large variety of nonlinear\nsystems. The simplest common model -- one can think of -- is linear, but linear\nsystems often fall short in accurately capturing the complex dynamics of\nnonlinear systems. In this work, we propose using quadratic systems as the\ncommon structure, inspired by the lifting principle. According to this\nprinciple, smooth nonlinear systems can be expressed as quadratic systems in\nsuitable coordinates without approximation errors. However, finding these\ncoordinates solely from data is challenging. Here, we leverage deep learning to\nidentify such lifted coordinates using only data, enabling a quadratic\ndynamical system to describe the system's dynamics. Additionally, we discuss\nthe asymptotic stability of these quadratic dynamical systems. We illustrate\nthe approach using data collected from various numerical examples,\ndemonstrating its superior performance with the existing well-known techniques.",
        "link": "http://arxiv.org/pdf/2211.00357v2"
    },
    {
        "title": "Structural Inference of Networked Dynamical Systems with Universal Differential Equations",
        "year": "July 2022",
        "date": "2022-07-11",
        "authors": "James Koch, Zhao Chen, Aaron Tuor, Jan Drgona, Draguna Vrabie",
        "abstract": "Networked dynamical systems are common throughout science in engineering;\ne.g., biological networks, reaction networks, power systems, and the like. For\nmany such systems, nonlinearity drives populations of identical (or\nnear-identical) units to exhibit a wide range of nontrivial behaviors, such as\nthe emergence of coherent structures (e.g., waves and patterns) or otherwise\nnotable dynamics (e.g., synchrony and chaos). In this work, we seek to infer\n(i) the intrinsic physics of a base unit of a population, (ii) the underlying\ngraphical structure shared between units, and (iii) the coupling physics of a\ngiven networked dynamical system given observations of nodal states. These\ntasks are formulated around the notion of the Universal Differential Equation,\nwhereby unknown dynamical systems can be approximated with neural networks,\nmathematical terms known a priori (albeit with unknown parameterizations), or\ncombinations of the two. We demonstrate the value of these inference tasks by\ninvestigating not only future state predictions but also the inference of\nsystem behavior on varied network topologies. The effectiveness and utility of\nthese methods is shown with their application to canonical networked nonlinear\ncoupled oscillators.",
        "link": "http://arxiv.org/pdf/2207.04962v1"
    },
    {
        "title": "Multisymplectic Formulation of Deep Learning Using Mean--Field Type Control and Nonlinear Stability of Training Algorithm",
        "year": "July 2022",
        "date": "2022-07-07",
        "authors": "Nader Ganaba",
        "abstract": "As it stands, a robust mathematical framework to analyse and study various\ntopics in deep learning is yet to come to the fore. Nonetheless, viewing deep\nlearning as a dynamical system allows the use of established theories to\ninvestigate the behaviour of deep neural networks. In order to study the\nstability of the training process, in this article, we formulate training of\ndeep neural networks as a hydrodynamics system, which has a multisymplectic\nstructure. For that, the deep neural network is modelled using a stochastic\ndifferential equation and, thereby, mean-field type control is used to train\nit. The necessary conditions for optimality of the mean--field type control\nreduce to a system of Euler-Poincare equations, which has the a similar\ngeometric structure to that of compressible fluids. The mean-field type control\nis solved numerically using a multisymplectic numerical scheme that takes\nadvantage of the underlying geometry. Moreover, the numerical scheme, yields an\napproximated solution which is also an exact solution of a hydrodynamics system\nwith a multisymplectic structure and it can be analysed using backward error\nanalysis. Further, nonlinear stability yields the condition for selecting the\nnumber of hidden layers and the number of nodes per layer, that makes the\ntraining stable while approximating the solution of a residual neural network\nwith a number of hidden layers approaching infinity.",
        "link": "http://arxiv.org/pdf/2207.12242v1"
    },
    {
        "title": "Learning Fine Scale Dynamics from Coarse Observations via Inner Recurrence",
        "year": "June 2022",
        "date": "2022-06-03",
        "authors": "Victor Churchill, Dongbin Xiu",
        "abstract": "Recent work has focused on data-driven learning of the evolution of unknown\nsystems via deep neural networks (DNNs), with the goal of conducting long term\nprediction of the dynamics of the unknown system. In many real-world\napplications, data from time-dependent systems are often collected on a time\nscale that is coarser than desired, due to various restrictions during the data\nacquisition process. Consequently, the observed dynamics can be severely\nunder-sampled and do not reflect the true dynamics of the underlying system.\nThis paper presents a computational technique to learn the fine-scale dynamics\nfrom such coarsely observed data. The method employs inner recurrence of a DNN\nto recover the fine-scale evolution operator of the underlying system. In\naddition to mathematical justification, several challenging numerical examples,\nincluding unknown systems of both ordinary and partial differential equations,\nare presented to demonstrate the effectiveness of the proposed method.",
        "link": "http://arxiv.org/pdf/2206.01807v1"
    },
    {
        "title": "Scalable algorithms for physics-informed neural and graph networks",
        "year": "May 2022",
        "date": "2022-05-16",
        "authors": "Khemraj Shukla, Mengjia Xu, Nathaniel Trask, George Em Karniadakis",
        "abstract": "Physics-informed machine learning (PIML) has emerged as a promising new\napproach for simulating complex physical and biological systems that are\ngoverned by complex multiscale processes for which some data are also\navailable. In some instances, the objective is to discover part of the hidden\nphysics from the available data, and PIML has been shown to be particularly\neffective for such problems for which conventional methods may fail. Unlike\ncommercial machine learning where training of deep neural networks requires big\ndata, in PIML big data are not available. Instead, we can train such networks\nfrom additional information obtained by employing the physical laws and\nevaluating them at random points in the space-time domain. Such\nphysics-informed machine learning integrates multimodality and multifidelity\ndata with mathematical models, and implements them using neural networks or\ngraph networks. Here, we review some of the prevailing trends in embedding\nphysics into machine learning, using physics-informed neural networks (PINNs)\nbased primarily on feed-forward neural networks and automatic differentiation.\nFor more complex systems or systems of systems and unstructured data, graph\nneural networks (GNNs) present some distinct advantages, and here we review how\nphysics-informed learning can be accomplished with GNNs based on graph exterior\ncalculus to construct differential operators; we refer to these architectures\nas physics-informed graph networks (PIGNs). We present representative examples\nfor both forward and inverse problems and discuss what advances are needed to\nscale up PINNs, PIGNs and more broadly GNNs for large-scale engineering\nproblems.",
        "link": "http://arxiv.org/pdf/2205.08332v1"
    },
    {
        "title": "On Neural Differential Equations",
        "year": "February 2022",
        "date": "2022-02-04",
        "authors": "Patrick Kidger",
        "abstract": "The conjoining of dynamical systems and deep learning has become a topic of\ngreat interest. In particular, neural differential equations (NDEs) demonstrate\nthat neural networks and differential equation are two sides of the same coin.\nTraditional parameterised differential equations are a special case. Many\npopular neural network architectures, such as residual networks and recurrent\nnetworks, are discretisations.\n  NDEs are suitable for tackling generative problems, dynamical systems, and\ntime series (particularly in physics, finance, ...) and are thus of interest to\nboth modern machine learning and traditional mathematical modelling. NDEs offer\nhigh-capacity function approximation, strong priors on model space, the ability\nto handle irregular data, memory efficiency, and a wealth of available theory\non both sides.\n  This doctoral thesis provides an in-depth survey of the field.\n  Topics include: neural ordinary differential equations (e.g. for hybrid\nneural/mechanistic modelling of physical systems); neural controlled\ndifferential equations (e.g. for learning functions of irregular time series);\nand neural stochastic differential equations (e.g. to produce generative models\ncapable of representing complex stochastic dynamics, or sampling from complex\nhigh-dimensional distributions).\n  Further topics include: numerical methods for NDEs (e.g. reversible\ndifferential equations solvers, backpropagation through differential equations,\nBrownian reconstruction); symbolic regression for dynamical systems (e.g. via\nregularised evolution); and deep implicit models (e.g. deep equilibrium models,\ndifferentiable optimisation).\n  We anticipate this thesis will be of interest to anyone interested in the\nmarriage of deep learning with dynamical systems, and hope it will provide a\nuseful reference for the current state of the art.",
        "link": "http://arxiv.org/pdf/2202.02435v1"
    }
]