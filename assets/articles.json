{
    "last_month": [
        {
            "title": "ParallelFlow: Parallelizing Linear Transformers via Flow Discretization",
            "year": "April 2025",
            "date": "2025-04-01",
            "authors": "Nicola Muca Cirone, Cristopher Salvi",
            "abstract": "We present a theoretical framework for analyzing linear attention models\nthrough matrix-valued state space models (SSMs). Our approach, Parallel Flows,\nprovides a perspective that systematically decouples temporal dynamics from\nimplementation constraints, enabling independent analysis of critical\nalgorithmic components: chunking, parallelization, and information aggregation.\nCentral to this framework is the reinterpretation of chunking procedures as\ncomputations of the flows governing system dynamics. This connection\nestablishes a bridge to mathematical tools from rough path theory, opening the\ndoor to new insights into sequence modeling architectures. As a concrete\napplication, we analyze DeltaNet in a generalized low-rank setting motivated by\nrecent theoretical advances. Our methods allow us to design simple, streamlined\ngeneralizations of hardware-efficient algorithms present in the literature, and\nto provide completely different ones, inspired by rough paths techniques, with\nprovably lower complexity. This dual contribution demonstrates how principled\ntheoretical analysis can both explain existing practical methods and inspire\nfundamentally new computational approaches.",
            "link": "http://arxiv.org/pdf/2504.00492v1"
        },
        {
            "title": "Probabilistic Forecasting for Dynamical Systems with Missing or Imperfect Data",
            "year": "March 2025",
            "date": "2025-03-15",
            "authors": "Siddharth Rout, Eldad Haber, St√©phane Gaudreault",
            "abstract": "The modeling of dynamical systems is essential in many fields, but applying\nmachine learning techniques is often challenging due to incomplete or noisy\ndata. This study introduces a variant of stochastic interpolation (SI) for\nprobabilistic forecasting, estimating future states as distributions rather\nthan single-point predictions. We explore its mathematical foundations and\ndemonstrate its effectiveness on various dynamical systems, including the\nchallenging WeatherBench dataset.",
            "link": "http://arxiv.org/pdf/2503.12273v1"
        },
        {
            "title": "Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models",
            "year": "March 2025",
            "date": "2025-03-27",
            "authors": "Tom Kempton, Stuart Burrell",
            "abstract": "Advances in hardware and language model architecture have spurred a\nrevolution in natural language generation. However, autoregressive models\ncompute probability distributions over next-token choices, and sampling from\nthese distributions, known as decoding, has received significantly less\nattention than other design choices. Existing decoding strategies are largely\nbased on heuristics, resulting in methods that are hard to apply or improve in\na principled manner. We develop the theory of decoding strategies for language\nmodels by expressing popular decoding algorithms as equilibrium states in the\nlanguage of ergodic theory and stating the functions they optimize. Using this,\nwe analyze the effect of the local normalization step of top-k, nucleus, and\ntemperature sampling, used to make probabilities sum to one. We argue that\nlocal normalization distortion is a fundamental defect of decoding strategies\nand quantify the size of this distortion and its effect on mathematical proxies\nfor the quality and diversity of generated text. Contrary to the prevailing\nexplanation, we argue that the major cause of the under-performance of top-k\nsampling relative to nucleus sampling is local normalization distortion. This\nyields conclusions for the future design of decoding algorithms and the\ndetection of machine-generated text.",
            "link": "http://arxiv.org/pdf/2503.21929v1"
        },
        {
            "title": "A Low-complexity Structured Neural Network to Realize States of Dynamical Systems",
            "year": "March 2025",
            "date": "2025-03-31",
            "authors": "Hansaka Aluvihare, Levi Lingsch, Xianqi Li, Sirani M. Perera",
            "abstract": "Data-driven learning is rapidly evolving and places a new perspective on\nrealizing state-space dynamical systems. However, dynamical systems derived\nfrom nonlinear ordinary differential equations (ODEs) suffer from limitations\nin computational efficiency. Thus, this paper stems from data-driven learning\nto advance states of dynamical systems utilizing a structured neural network\n(StNN). The proposed learning technique also seeks to identify an optimal,\nlow-complexity operator to solve dynamical systems, the so-called Hankel\noperator, derived from time-delay measurements. Thus, we utilize the StNN based\non the Hankel operator to solve dynamical systems as an alternative to existing\ndata-driven techniques. We show that the proposed StNN reduces the number of\nparameters and computational complexity compared with the conventional neural\nnetworks and also with the classical data-driven techniques, such as Sparse\nIdentification of Nonlinear Dynamics (SINDy) and Hankel Alternative view of\nKoopman (HAVOK), which is commonly known as delay-Dynamic Mode\nDecomposition(DMD) or Hankel-DMD. More specifically, we present numerical\nsimulations to solve dynamical systems utilizing the StNN based on the Hankel\noperator beginning from the fundamental Lotka-Volterra model, where we compare\nthe StNN with the LEarning Across Dynamical Systems (LEADS), and extend our\nanalysis to highly nonlinear and chaotic Lorenz systems, comparing the StNN\nwith conventional neural networks, SINDy, and HAVOK. Hence, we show that the\nproposed StNN paves the way for realizing state-space dynamical systems with a\nlow-complexity learning algorithm, enabling prediction and understanding of\nfuture states.",
            "link": "http://arxiv.org/pdf/2503.23697v1"
        },
        {
            "title": "A Comparison of Parametric Dynamic Mode Decomposition Algorithms for Thermal-Hydraulics Applications",
            "year": "March 2025",
            "date": "2025-03-31",
            "authors": "Stefano Riva, Andrea Missaglia, Carolina Introini, In Cheol Bang, Antonio Cammi",
            "abstract": "In recent years, algorithms aiming at learning models from available data\nhave become quite popular due to two factors: 1) the significant developments\nin Artificial Intelligence techniques and 2) the availability of large amounts\nof data. Nevertheless, this topic has already been addressed by methodologies\nbelonging to the Reduced Order Modelling framework, of which perhaps the most\nfamous equation-free technique is Dynamic Mode Decomposition. This algorithm\naims to learn the best linear model that represents the physical phenomena\ndescribed by a time series dataset: its output is a best state operator of the\nunderlying dynamical system that can be used, in principle, to advance the\noriginal dataset in time even beyond its span. However, in its standard\nformulation, this technique cannot deal with parametric time series, meaning\nthat a different linear model has to be derived for each parameter realization.\nResearch on this is ongoing, and some versions of a parametric Dynamic Mode\nDecomposition already exist. This work contributes to this research field by\ncomparing the different algorithms presently deployed and assessing their\nadvantages and shortcomings compared to each other. To this aim, three\ndifferent thermal-hydraulics problems are considered: two benchmark 'flow over\ncylinder' test cases at diverse Reynolds numbers, whose datasets are,\nrespectively, obtained with the FEniCS finite element solver and retrieved from\nthe CFDbench dataset, and the DYNASTY experimental facility operating at\nPolitecnico di Milano, which studies the natural circulation established by\ninternally heated fluids for Generation IV nuclear applications, whose dataset\nwas generated using the RELAP5 nodal solver.",
            "link": "http://arxiv.org/pdf/2503.24205v1"
        }
    ],
    "last_year": [
        {
            "title": "Predator Prey Scavenger Model using Holling's Functional Response of Type III and Physics-Informed Deep Neural Networks",
            "year": "December 2024",
            "date": "2024-12-24",
            "authors": "Aneesh Panchal, Kirti Beniwal, Vivek Kumar",
            "abstract": "Nonlinear mathematical models introduce the relation between various physical\nand biological interactions present in nature. One of the most famous models is\nthe Lotka-Volterra model which defined the interaction between predator and\nprey species present in nature. However, predators, scavengers, and prey\npopulations coexist in a natural system where scavengers can additionally rely\non the dead bodies of predators present in the system. Keeping this in mind,\nthe formulation and simulation of the predator prey scavenger model is\nintroduced in this paper. For the predation response, respective prey species\nare assumed to have Holling's functional response of type III. The proposed\nmodel is tested for various simulations and is found to be showing satisfactory\nresults in different scenarios. After simulations, the American forest dataset\nis taken for parameter estimation which imitates the real-world case. For\nparameter estimation, a physics-informed deep neural network is used with the\nAdam backpropagation method which prevents the avalanche effect in trainable\nparameters updation. For neural networks, mean square error and\nphysics-informed informed error are considered. After the neural network, the\nhence-found parameters are fine-tuned using the\nBroyden-Fletcher-Goldfarb-Shanno algorithm. Finally, the hence-found parameters\nusing a natural dataset are tested for stability using Jacobian stability\nanalysis. Future research work includes minimization of error induced by\nparameters, bifurcation analysis, and sensitivity analysis of the parameters.",
            "link": "http://arxiv.org/pdf/2412.18344v1"
        },
        {
            "title": "Reservoir Computing with Generalized Readout based on Generalized Synchronization",
            "year": "May 2024",
            "date": "2024-05-03",
            "authors": "Akane Ookubo, Masanobu Inubushi",
            "abstract": "Reservoir computing is a machine learning framework that exploits nonlinear\ndynamics, exhibiting significant computational capabilities. One of the\ndefining characteristics of reservoir computing is its low cost and\nstraightforward training algorithm, i.e. only the readout, given by a linear\ncombination of reservoir variables, is trained. Inspired by recent mathematical\nstudies based on dynamical system theory, in particular generalized\nsynchronization, we propose a novel reservoir computing framework with\ngeneralized readout, including a nonlinear combination of reservoir variables.\nThe first crucial advantage of using the generalized readout is its\nmathematical basis for improving information processing capabilities. Secondly,\nit is still within a linear learning framework, which preserves the original\nstrength of reservoir computing. In summary, the generalized readout is\nnaturally derived from mathematical theory and allows the extraction of useful\nbasis functions from reservoir dynamics without sacrificing simplicity. In a\nnumerical study, we find that introducing the generalized readout leads to a\nsignificant improvement in accuracy and an unexpected enhancement in robustness\nfor the short- and long-term prediction of Lorenz chaos, with a particular\nfocus on how to harness low-dimensional reservoir dynamics. A novel way and its\nadvantages for physical implementations of reservoir computing with generalized\nreadout are briefly discussed.",
            "link": "http://arxiv.org/pdf/2405.14885v1"
        },
        {
            "title": "ADAM-SINDy: An Efficient Optimization Framework for Parameterized Nonlinear Dynamical System Identification",
            "year": "October 2024",
            "date": "2024-10-21",
            "authors": "Siva Viknesh, Younes Tatari, Amirhossein Arzani",
            "abstract": "Identifying dynamical systems characterized by nonlinear parameters presents\nsignificant challenges in deriving mathematical models that enhance\nunderstanding of physics. Traditional methods, such as Sparse Identification of\nNonlinear Dynamics (SINDy) and symbolic regression, can extract governing\nequations from observational data; however, they also come with distinct\nadvantages and disadvantages. This paper introduces a novel method within the\nSINDy framework, termed ADAM-SINDy, which synthesizes the strengths of\nestablished approaches by employing the ADAM optimization algorithm. This\nfacilitates the simultaneous optimization of nonlinear parameters and\ncoefficients associated with nonlinear candidate functions, enabling precise\nparameter estimation without requiring prior knowledge of nonlinear\ncharacteristics such as trigonometric frequencies, exponential bandwidths, or\npolynomial exponents, thereby addressing a key limitation of SINDy. Through an\nintegrated global optimization, ADAM-SINDy dynamically adjusts all unknown\nvariables in response to data, resulting in an adaptive identification\nprocedure that reduces the sensitivity to the library of candidate functions.\nThe performance of the ADAM-SINDy methodology is demonstrated across a spectrum\nof dynamical systems, including benchmark coupled nonlinear ordinary\ndifferential equations such as oscillators, chaotic fluid flows, reaction\nkinetics, pharmacokinetics, as well as nonlinear partial differential equations\n(wildfire transport). The results demonstrate significant improvements in\nidentifying parameterized dynamical systems and underscore the importance of\nconcurrently optimizing all parameters, particularly those characterized by\nnonlinear parameters. These findings highlight the potential of ADAM-SINDy to\nextend the applicability of the SINDy framework in addressing more complex\nchallenges in dynamical system identification.",
            "link": "http://arxiv.org/pdf/2410.16528v2"
        },
        {
            "title": "Generalization of the Painlev√© Property and Existence and Uniqueness in Fractional Differential Equations",
            "year": "November 2024",
            "date": "2024-11-28",
            "authors": "Micha≈Ç Fiedorowicz",
            "abstract": "In this paper, the Painlev\\'e property to fractional differential equations\n(FDEs) are extended and the existence and uniqueness theorems for both linear\nand nonlinear FDEs are established. The results contribute to the research of\nintegrability and solvability in the context of fractional calculus, which has\nsignificant implications in various fields such as physics, engineering, and\napplied sciences. By bridging the gap between pure mathematical theory and\npractical applications, this work provides a foundational understanding that\ncan be utilized in modeling phenomena exhibiting memory and hereditary\nproperties.",
            "link": "http://arxiv.org/pdf/2411.19411v1"
        },
        {
            "title": "ParallelFlow: Parallelizing Linear Transformers via Flow Discretization",
            "year": "April 2025",
            "date": "2025-04-01",
            "authors": "Nicola Muca Cirone, Cristopher Salvi",
            "abstract": "We present a theoretical framework for analyzing linear attention models\nthrough matrix-valued state space models (SSMs). Our approach, Parallel Flows,\nprovides a perspective that systematically decouples temporal dynamics from\nimplementation constraints, enabling independent analysis of critical\nalgorithmic components: chunking, parallelization, and information aggregation.\nCentral to this framework is the reinterpretation of chunking procedures as\ncomputations of the flows governing system dynamics. This connection\nestablishes a bridge to mathematical tools from rough path theory, opening the\ndoor to new insights into sequence modeling architectures. As a concrete\napplication, we analyze DeltaNet in a generalized low-rank setting motivated by\nrecent theoretical advances. Our methods allow us to design simple, streamlined\ngeneralizations of hardware-efficient algorithms present in the literature, and\nto provide completely different ones, inspired by rough paths techniques, with\nprovably lower complexity. This dual contribution demonstrates how principled\ntheoretical analysis can both explain existing practical methods and inspire\nfundamentally new computational approaches.",
            "link": "http://arxiv.org/pdf/2504.00492v1"
        }
    ],
    "last_5_years": [
        {
            "title": "A mathematical perspective on Transformers",
            "year": "December 2023",
            "date": "2023-12-17",
            "authors": "Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, Philippe Rigollet",
            "abstract": "Transformers play a central role in the inner workings of large language\nmodels. We develop a mathematical framework for analyzing Transformers based on\ntheir interpretation as interacting particle systems, which reveals that\nclusters emerge in long time. Our study explores the underlying theory and\noffers new perspectives for mathematicians as well as computer scientists.",
            "link": "http://arxiv.org/pdf/2312.10794v4"
        },
        {
            "title": "Coupled and Uncoupled Dynamic Mode Decomposition in Multi-Compartmental Systems with Applications to Epidemiological and Additive Manufacturing Problems",
            "year": "October 2021",
            "date": "2021-10-12",
            "authors": "Alex Viguerie, Gabriel F. Barros, Mal√∫ Grave, Alessandro Reali, Alvaro L. G. A. Coutinho",
            "abstract": "Dynamic Mode Decomposition (DMD) is an unsupervised machine learning method\nthat has attracted considerable attention in recent years owing to its\nequation-free structure, ability to easily identify coherent spatio-temporal\nstructures in data, and effectiveness in providing reasonably accurate\npredictions for certain problems. Despite these successes, the application of\nDMD to certain problems featuring highly nonlinear transient dynamics remains\nchallenging. In such cases, DMD may not only fail to provide acceptable\npredictions but may indeed fail to recreate the data in which it was trained,\nrestricting its application to diagnostic purposes. For many problems in the\nbiological and physical sciences, the structure of the system obeys a\ncompartmental framework, in which the transfer of mass within the system moves\nwithin states. In these cases, the behavior of the system may not be accurately\nrecreated by applying DMD to a single quantity within the system, as proper\nknowledge of the system dynamics, even for a single compartment, requires that\nthe behavior of other compartments is taken into account in the DMD process. In\nthis work, we demonstrate, theoretically and numerically, that, when performing\nDMD on a fully coupled PDE system with compartmental structure, one may recover\nuseful predictive behavior, even when DMD performs poorly when acting\ncompartment-wise. We also establish that important physical quantities, as mass\nconservation, are maintained in the coupled-DMD extrapolation. The mathematical\nand numerical analysis suggests that DMD may be a powerful tool when applied to\nthis common class of problems. In particular, we show interesting numerical\napplications to a continuous delayed-SIRD model for Covid-19, and to a problem\nfrom additive manufacturing considering a nonlinear temperature field and the\nresulting change of material phase from powder, liquid, and solid states.",
            "link": "http://arxiv.org/pdf/2110.06375v1"
        },
        {
            "title": "Some open problems in low dimensional dynamical systems",
            "year": "December 2020",
            "date": "2020-12-04",
            "authors": "Armengol Gasull",
            "abstract": "The aim of this paper is to share with the mathematical community a list of\n33 problems that I have found along the years during my research. I believe\nthat it is worth to think about them and, hopefully, it will be possible either\nto solve some of the problems or to make some substantial progress. Many of\nthem are about planar differential equations but there are also questions about\nother mathematical aspects: Abel differential equations, difference equations,\nglobal asymptotic stability, geometrical questions, problems involving\npolynomials or some recreational problems with a dynamical component.",
            "link": "http://arxiv.org/pdf/2012.02524v1"
        },
        {
            "title": "On Neural Differential Equations",
            "year": "February 2022",
            "date": "2022-02-04",
            "authors": "Patrick Kidger",
            "abstract": "The conjoining of dynamical systems and deep learning has become a topic of\ngreat interest. In particular, neural differential equations (NDEs) demonstrate\nthat neural networks and differential equation are two sides of the same coin.\nTraditional parameterised differential equations are a special case. Many\npopular neural network architectures, such as residual networks and recurrent\nnetworks, are discretisations.\n  NDEs are suitable for tackling generative problems, dynamical systems, and\ntime series (particularly in physics, finance, ...) and are thus of interest to\nboth modern machine learning and traditional mathematical modelling. NDEs offer\nhigh-capacity function approximation, strong priors on model space, the ability\nto handle irregular data, memory efficiency, and a wealth of available theory\non both sides.\n  This doctoral thesis provides an in-depth survey of the field.\n  Topics include: neural ordinary differential equations (e.g. for hybrid\nneural/mechanistic modelling of physical systems); neural controlled\ndifferential equations (e.g. for learning functions of irregular time series);\nand neural stochastic differential equations (e.g. to produce generative models\ncapable of representing complex stochastic dynamics, or sampling from complex\nhigh-dimensional distributions).\n  Further topics include: numerical methods for NDEs (e.g. reversible\ndifferential equations solvers, backpropagation through differential equations,\nBrownian reconstruction); symbolic regression for dynamical systems (e.g. via\nregularised evolution); and deep implicit models (e.g. deep equilibrium models,\ndifferentiable optimisation).\n  We anticipate this thesis will be of interest to anyone interested in the\nmarriage of deep learning with dynamical systems, and hope it will provide a\nuseful reference for the current state of the art.",
            "link": "http://arxiv.org/pdf/2202.02435v1"
        },
        {
            "title": "Scalable algorithms for physics-informed neural and graph networks",
            "year": "May 2022",
            "date": "2022-05-16",
            "authors": "Khemraj Shukla, Mengjia Xu, Nathaniel Trask, George Em Karniadakis",
            "abstract": "Physics-informed machine learning (PIML) has emerged as a promising new\napproach for simulating complex physical and biological systems that are\ngoverned by complex multiscale processes for which some data are also\navailable. In some instances, the objective is to discover part of the hidden\nphysics from the available data, and PIML has been shown to be particularly\neffective for such problems for which conventional methods may fail. Unlike\ncommercial machine learning where training of deep neural networks requires big\ndata, in PIML big data are not available. Instead, we can train such networks\nfrom additional information obtained by employing the physical laws and\nevaluating them at random points in the space-time domain. Such\nphysics-informed machine learning integrates multimodality and multifidelity\ndata with mathematical models, and implements them using neural networks or\ngraph networks. Here, we review some of the prevailing trends in embedding\nphysics into machine learning, using physics-informed neural networks (PINNs)\nbased primarily on feed-forward neural networks and automatic differentiation.\nFor more complex systems or systems of systems and unstructured data, graph\nneural networks (GNNs) present some distinct advantages, and here we review how\nphysics-informed learning can be accomplished with GNNs based on graph exterior\ncalculus to construct differential operators; we refer to these architectures\nas physics-informed graph networks (PIGNs). We present representative examples\nfor both forward and inverse problems and discuss what advances are needed to\nscale up PINNs, PIGNs and more broadly GNNs for large-scale engineering\nproblems.",
            "link": "http://arxiv.org/pdf/2205.08332v1"
        }
    ],
    "all_time": [
        {
            "title": "A Model of Blood Flow in a Circulation Network",
            "year": "October 2002",
            "date": "2002-10-26",
            "authors": "Weihua Ruan, M. E. Clark, Meide Zhao, Anthony Curcio",
            "abstract": "We study a mathematical model of a blood circulation network which is a\ngeneralization of the coronary model proposed by Smith, Pullan and Hunter. We\nprove the existence and uniqueness of the solution to the initial-boundary\nvalue problem and discuss the continuity of dependence of the solution and its\nderivatives on initial, boundary and forcing functions and their derivatives.",
            "link": "http://arxiv.org/pdf/math/0210410v1"
        },
        {
            "title": "Modeling the influence of TH1 and TH2 type cells in autoimmune diseases",
            "year": "June 2000",
            "date": "2000-06-19",
            "authors": "Y. Louzoun, H. Atlan, I. R. Cohen",
            "abstract": "A sharp TH1/TH2 dichotomy has often been used to define the effects of\ncytokines on autoimmune diseases. However contradictory results in recent\nresearch indicate that the situation may be more complex. We build here a\nsimple mathematical model aimed at settling the contradictions. The model is\nbased on a neural network paradigm, and is applied using Partial Differential\nEquations (PDE). We show here that a TH1/TH2 paradigm is only an external view\nof a complex multivariate system.",
            "link": "http://arxiv.org/pdf/math/0006127v1"
        },
        {
            "title": "Gain-induced oscillations in blood pressure",
            "year": "August 1997",
            "date": "1997-08-16",
            "authors": "Roselyn M. Abbiw-Jackson, William Langford",
            "abstract": "\"Mayer waves\" are long-period (6 to 12 seconds) oscillations in arterial\nblood pressure, which have been observed and studied for more than 100 years in\nthe cardiovascular system of humans and other mammals. A mathematical model of\nthe human cardiovascular system is presented, incorporating parameters\nrelevantto the onset of Mayer waves. The model is analyzed using methods of\nLyapunov stability and Hopf bifurcation theory. The analysis shows that\nincrease in the gain of the baroreflex feedback loop controlling venous volume\nmay lead to the onset of oscillations, while changes in the other parameters\nconsidered do not affect stability of the equilibrium state. The results agree\nwith clinical observations of Mayer waves in human subjects, both in the period\nof the oscillations and in the observed age-dependence of Mayer waves. This\nleads to a proposed explanation of their occurrence, namely that Mayer waves\nare a \"gain-induced instability\".",
            "link": "http://arxiv.org/pdf/math/9708211v1"
        },
        {
            "title": "Rhythms of the nervous system: mathematical themes and variations",
            "year": "May 2003",
            "date": "2003-05-01",
            "authors": "Nancy Kopell",
            "abstract": "The nervous system displays a variety of rhythms in both waking and sleep.\nThese rhythms have been closely associated with different behavioral and\ncognitive states, but it is still unknown how the nervous system makes use of\nthese rhythms to perform functionally important tasks. To address those\nquestions, it is first useful to understood in a mechanistic way the origin of\nthe rhythms, their interactions, the signals which create the transitions among\nrhythms, and the ways in which rhythms filter the signals to a network of\nneurons. This talk discusses how dynamical systems have been used to\ninvestigate the origin, properties and interactions of rhythms in the nervous\nsystem. It focuses on how the underlying physiology of the cells and synapses\nof the networks shape the dynamics of the network in different contexts,\nallowing the variety of dynamical behaviors to be displayed by the same\nnetwork. The work is presented using a series of related case studies on\ndifferent rhythms. These case studies are chosen to highlight mathematical\nissues, and suggest further mathematical work to be done. The topics include:\ndifferent roles of excitation and inhibition in creating synchronous assemblies\nof cells, different kinds of building blocks for neural oscillations, and\ntransitions among rhythms. The mathematical issues include reduction of large\nnetworks to low dimensional maps, role of noise, global bifurcations, use of\nprobabilistic formulations.",
            "link": "http://arxiv.org/pdf/math/0305013v1"
        },
        {
            "title": "Destruction of CD4 T Lymphocytes Alone Cannot Account for their Long-term Decrease in AIDS",
            "year": "August 2000",
            "date": "2000-08-07",
            "authors": "Yoram Louzoun, Irun. R. Cohen, Henri Atlan",
            "abstract": "Following previous models describing a quasi steady state (QSS) for the\nevolution of HIV infection and AIDS, we have developed a larger formalism\nsimulating the long-term evolution of the QSS We show that the long-term\nevolution of AIDS cannot be explained by the destruction alone of CD4 T cells,\neither directly or indirectly. The destruction of CD4 T cells can lead only to\na QSS with a lower concentration of CD4 T cells, but CD4 destruction cannot\ngenerate the sustained long-term decrease in T cells leading to AIDS. We here\nsuggest some workable explanations.",
            "link": "http://arxiv.org/pdf/math/0008052v1"
        }
    ]
}